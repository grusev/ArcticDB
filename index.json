{"project": "ArcticDB", "project_url": "http://arcticdb.io/", "show_commit_url": "https://github.com/man-group/ArcticDB/commit/", "hash_length": 8, "revision_to_hash": {"953": "724ecccd67548b5bbd4acd75bf46b317055448d6", "954": "c696f598cb7c9e0ae77227007a86e1868be274ac", "956": "1c5a41918fd650471750062295c9369e1679ebd4", "957": "89fc8bb90dcc29fed6e39ca3ddd5b9824c64b2b2", "958": "9a4fadce8190395c4a873037babce412ced6f987", "960": "8240ab2220494344868d6a1031b58e3679c16e9b", "961": "b2c820c700eeb48514c76fd329995217c17118f9", "963": "cd632fc84df936930989dc3e77ca541927cd8d21", "966": "13b807c06ab950134a861efc8f8fdfa46076a6db", "967": "65cb44d99e29a1dcedad76fbcda6af0e6ce6c7eb", "969": "72f3730d81b3d1d86fd98832c825936f4a290e5b", "970": "ac846a89dc11130c590798c4d8fc1ae690a37783", "971": "5af60035e3eb9b74949ba1791e6e83c8d2cc6699", "976": "e0aa4eaa54ab27907594bf4c6038f64bebf37509", "977": "bf1d7885241e145b7e2c95b0ac49d107f4c873d3", "981": "356972d3c4e78efb84602d3f764dc58bf77d5bbf", "982": "911a55c5543626d7c0da39e6db2e7d39d716cf60", "985": "ff4309d06fe2fa506d574264dfccfcb9e7306a69", "988": "7205a86a8ce0ef45066d78f0357f5e08031a157b", "989": "9e3a47dd0efd27e37a4047142612b17da4cd9ddc", "992": "03ff0dc0cde52481a3f2ab2243c35d3c26813322", "993": "b367e7e9b4bfcb1c848a1db92352e8de2fd91bd9", "998": "1cd7d33bf88f7148ae0aad8f525d5f0f4392147c", "1003": "c81f8ad2b8f0831cfa855e6b3dc954b180c4c941", "1004": "9b2cbb9f1bf58b70f09370cd74de691083f8e6f6", "1005": "8cf8279b45649f03582202e169aeb6a0f2978732", "1006": "31853f18bd5b4ec291515860f9f572d01d6e67b6", "1007": "66a5d19c4a2c3b2423b52a68f984ca8d9b52afce", "1009": "dc6764fb5ef2d159f9f2e35bcf9c68493d94b140", "1010": "cb607222a64cc53012c65ebfbdbbb98090c1125b", "1012": "dfeb11207051437388e4574ab2b9636893415150", "1013": "083e0f602910ad86524d719cab1ae367eb06a5a5", "1015": "7bf8b4aa087ed4226d99111ceab18095da1d358c", "1017": "d0f7487f162fcc22239ecd84ae705773cdf9ef51", "1022": "51b8e2a327c9facebe27289946a6d80c38a67982", "1026": "a21da2371ca7875f39ff6d14462ed16d0520b9c9", "1038": "b9aabf5942884764f6c641cd9429a5d6f47cd6a1", "1040": "86e9f769e3fb555f0037674672c75cd3120132f7", "1041": "bbf594f8c200231bcf50a81c64e396bb42d19cda", "1045": "2881422c59fc8dd0f070daec9d3f495627f68b06", "1049": "6fc2aaa50fff579b178413a7bfd4b5c9c811e214", "1052": "0fc441cf64d46994cdb5b766e68c3d81b60e0086", "1053": "89817db81728b323ca296ba18a957c8aacfe11bb", "1057": "28feeb340c52664d5f9aec1363f47a499a0bf40e", "1061": "ec65b8e5f2d3bd556ecd1e3d73cb23705cafd7e8", "1062": "498c3311dc08f5c8872ccac0ef3dfb278776699c", "1065": "102b1c0ef09db96014a65d6fea007854744e0906", "1068": "83deb987adf6eda8fd1381b3834ddd7e702e051b", "1069": "e0456876ba5c0f749756f6de22c46d0d923f082d", "1071": "6ba2ff168b556044b54a8d6c37ee8e7ff17acf75", "1075": "4d092838343e729ef7579c947b5e8b7dc83bb563", "1079": "7df641fba906a2cba0ce406d4161dff27f6b3946", "1081": "17ef7f0c0b3970227b5fada52e7e2832615ef8d0", "1085": "8461ff17bb6f4768dbed554e52d55afc7b8d681f", "1089": "1bb57d7b02fb2a101831dc9ec1a6faf1dd3160ee", "1092": "bdf44d6a0268c55fcd5d3da619812325e0e8b44f", "1097": "2cad4622122e730032271a0e936a8c590f611cd4", "1100": "a398c24a1f875cc848d016fbfdc0347f33e81d63", "1102": "c6a0567bb3f64537c350cebb23fd97043f6e577e", "1103": "56e88c19073613255cf3b606f88de6652af3987f", "1105": "07fcc09413230f136b3903deaa9262ce466bb6de", "1108": "3109deeec996ccc1c4d845c7fea9d7ab9edf2746", "1110": "52cdc81b7f175ad410b4a3626b366e3ab1aec3a1", "1111": "392c9abbfee136203f31fcd4e82db59ea72516a2", "1112": "008fa54cfe123c9ac4276643ebb5e22126d7a3c0", "1113": "b157c924171e5bfdd4e0a2658c611917037bcaa5", "1114": "d607f94ff380497dbbf2e3811cd77d3c6113ba80", "1115": "5bae9ab9726635369a70a9eea95ded65215706f5", "1116": "d4e6854093138cb612a6c62b19c19a0b17d9b5b1", "1117": "70400ec7517aaf3e3b9ed598661c73c2f1a50cb1", "1119": "aa1eba344cd78605c537b37c3579664da4bc6258", "1122": "63ef514ff04a515731a0ae4b9c2eb92c1c125a8d", "1123": "9fe62421b97457f994f8d0188eebf0002038295b", "1124": "8669ceee98779f78c76601272abd5663f9406f2c", "1132": "25ad2a3fded2c79b035e6bc346745863f81d3d57", "1136": "db3318090cec67f6da0812935dda29fff2605556", "1138": "1596c8012938bc59e0433cb56797be406cf8e445", "1142": "3419d8f2ade78432ea180d68008e2c1a375ad08a", "1143": "b20a6776b02e9c8120d748e85d648c26b59f0a33", "1144": "a3f5330e47fff803fe830b416d166685ee349998", "1145": "8e4b241088f4ffd1778b0a60b935d0defe695277", "1149": "af83cfaf839507db02312e0f7402e77b0e2c87ed", "1150": "dba420f587bb8d760a4d7c89fdc60ad07a971f7f", "1151": "4719f9e87fef59b177068e6316bbc0a3eff98872", "1153": "717449fef3c05c55a02f486ee56c25f09f80e43a", "1161": "b0e80e3c39ac95c3fc57ab3f040a73257cbf65fc", "1162": "1a56956ff0be53b4202dd7fbbfa4a86afd172d8e", "1163": "2032e351d53b3008165c1d6bca1157bcf339ddc8", "1164": "56d8611b23361bebbdda20973fddf034573b6bdf", "1165": "9d073c626bd55011e1af924bd4aa5e609e277795", "1167": "d2ea6b112ca5c80a2ae89161c9a0067e61821169", "1168": "f4f2ddaa15a7e60c6895c2c3cb84f133f4a3adb3", "1170": "50f1d04a514aeaaa85b0e4f5d6c35a121f6b927a", "1173": "a950d1333e34166914ba62e1a4214dbb59223675", "1174": "84678ecb47304bac0307916f75b36a5d2cc8fea3", "1176": "940f2e70ce361397a9d4abd4acc506aec2c9046a", "1180": "f24449b1aeeeb47e97f217afda8d517bac480e51", "1182": "63b8b0c553633a928e6b5cf2674913f5753a9c64", "1183": "c20c189fa835945b1cd61f48952f1dc9709638bc", "1184": "fc1d8f24c5f38bc26c01f94578ed302ed33a1164", "1185": "c9e5b04672c208f238b97825db3f6a22cff6d938", "1186": "97644bf95ff7303d0c1c10511384c303e954bf4c", "1188": "fd0ec5d0800d9e87de32f9d07a1dc1dcbc137ed0", "1189": "8ee376319e69e82ad7e2ce1ce02758b71905c668", "1190": "0b9618581fafb726e65e56778e5011ec36380473", "1191": "14ca39fb569343bd71f0647561d84173eefe1b50", "1192": "ef93cdd5fd804f6285a70a10307f230740d16e45", "1194": "6abf878cea50847ea4321ee3dc9c7c52e0a18420", "1195": "86449ec46d107d52baaacb62cfd91b5b6922ca6f", "1197": "b0c174c5b888e2c310734382d806a07d54813421", "1198": "8d9bb180934ca31ee064720011a636ab755014c1", "1204": "cc4f51bec8bea6fc42261fab15b8cc378882f4db", "1209": "f2e48c4ba5a25177ba233fff3e35428e470153b3", "1210": "c27b76ee98dbbc5c0a8f6c94df26a1cdb49220ca", "1211": "d19366dc31494df354a9023a5dca0fc2a0ec37ca", "1212": "3cc729e591af5fa5b869f6b84d77b28a09736567", "1213": "b07cfbfdf910b0ce78d50ec32cf808255ce51692", "1214": "a38ac9da675fd65334c8378a611e50e7ed612ffd", "1220": "26a7ce88a945397d0cd71eb8516b7ac742828fe8", "1221": "5e6f279ab771ea123ee4a5978005d0d6f0beb707", "1222": "f6f0e5fe263fac68000f72ae791ae53e05e3b669", "1228": "2a6b3cdc3480126f6ce4b7c877339f412aee6444", "1229": "4c12a0ba6fb8efec6c0e8bfd540dea06f2857313", "1230": "6698859b48cd6d93937950a06ba6f34af4be257e", "1231": "cb5051d530699806a587938f123e852e2fa766d4", "1235": "09a23a1028cf1645454d7dc6b660df3c630fb0db", "1238": "9b3eed2cf2ab2024dd4be1c351b1117046aee84b", "1239": "a02f90f7582ef69afc2336e5c04182c9230afd53", "1240": "b81fdc1be84257b474e4dfaf5624b3ca5bfa7cdc", "1241": "860e1527e730c6b9d5266e7665f3306e6f40bfe9", "1242": "d2e568e12cc5b7ee90b622581171510fc8b90fa0", "1243": "1a012f2c69b73cba5227046f5cea009267d7fdd5", "1244": "ad0b3b4f4d370635d607d4200407822a57cb6592", "1245": "f9d955f71fbb7439ddce8724dda5ec8e23f6c171", "1246": "6931d3f67487dcf9f94b3cbd5bf6502818a210ff", "1247": "044ba062a50f35b6cd99eb421d3e6366f672d719", "1248": "159d9308cde3a6d5591440021ebf9ea6bc183816", "1251": "ed9f4c434c461f8a923c9925c8a59d61dc3f879a", "1252": "2f6e03c51c8123ae7ce5374ea2d5b75cf5270249", "1253": "a28bf7fe0b6ce32b6bacdd5a29546d62a53c7a99", "1254": "7cdcebcf227ad7089d5414259aaa63ba984ba448", "1257": "feb9b29f096e607875dc44b1dabf32d258116b53", "1258": "32341ae7c5f283aeed2a7f92d38897287062e942", "1259": "d71a0bb585f48c29b4c9d3aa0a053fe35b4ec0af", "1262": "0c3fcfc90ac1276c2f39461df813d9d0a9fb8b0d", "1264": "f812539451bd20a9e592efefd2e2a270595dd495", "1265": "8502cbd849a95c2d342c519e931a41c211e34c51", "1268": "f759485d3c17036456a7faa56a54d7f51aeffa75", "1270": "7a641d76caaa9bb842d50333b4c837334aff6534", "1271": "f92a5ac78a11c9820acf0e1d423a1bc53058e7dd", "1272": "3fd6d7e9fc674ff03a44f85a746cf9a219c7f91f", "1273": "339258cf1f3e3728366021181c58e23765871e88", "1276": "6407f7dda31007ffde537cdf2d2e616497302619", "1278": "d8b83216bbcb839e91d4519ac54a3fe2431f409b", "1279": "56a2046ae747ce5dd92ae21a9047140b8773dc2b", "1280": "4f1b23ce8204f474e8b9f2e50a32f083fa957a7c", "1283": "451f8fc337f60e3540e94c6efc313f011c340af0", "1284": "757b4bd29e3a3de87a2265e2c021b9fb717854bc", "1287": "aa258b5de8e70196617cb17db7b68f0a833853ad", "1288": "d261361c07380893ecf2b5c709bee3e0d469c8f7", "1289": "27fdea488f1c94e9ab91442a4ac4d45b563ee82e", "1291": "734e50a2ce6abc7bdbc233eb19d142573a428e50", "1292": "4a2fc470e5f685536d5028b8d84dde1072e59f17", "1293": "958a6ffd4b83c725b6da4a99b3efadde102aa4cd", "1294": "1afd029c7d512edeb22625b0f10eac90cdaf2d55", "1295": "873ae6caa7e692bacb4a4a3103b3a12c5d69af02", "1296": "920391be22c4bda760ff44ac5f0a688b40745bbb", "1300": "c698c8b3a5a66cefb31271473f099b83aac88d4b", "1301": "4fb62800903c74b164f49f91f89c9790bd7aac88", "1308": "7513bf0c436044fcd6b09391b0a1d2655bdc3478", "1310": "50e766e78add5ba7fbf5bcce80a21ac1c05dd2f8", "1311": "25e4a079f3b3d005ee698d1878ad0a73704176a0", "1313": "f6b8933bd858994be1995b9a91c6aeb40720dd14", "1314": "79ec8d7fabed4efc4cee4c8a67e9bca881b5fd35", "1315": "f89fd125fe4fbc0e0eaca62953d7fd074441bfe6", "1316": "12b9acd769ad2debef00f2f0010ea7d53ccbb0fd", "1317": "0b58a88e4b6aa97eef3a2131aebda6e7a6411ae2", "1318": "dcdd654dcdd18e844bd450cbb5926c5a8ada6faf", "1319": "1e00cfc788b4756e441275b0266096436ec49628", "1320": "dc0d813cd33ad1e581931be9c4e42104a2b6b2e2", "1321": "ad0b1cf1d1d329129f71681f01fb7a673df569e4", "1322": "6c4e53dcbe8a8a803872a6af5e301292000b60eb", "1324": "ad6059bd176fb14d34702e22a617f39bcb15ebcd", "1334": "bdc5d2442dddb2f11c045d4e35b008e1db67f0a6", "1336": "c6e589408c9806ed7c9f7424f3dc3446c2389a61", "1337": "74d771d88cf99df68f8e91cc3617945767c110e9", "1338": "509fa087a7c8c1fa1b06eaac81887020d2532e6c", "1339": "080db00294341ba6ab5200da6111202a7e199b93", "1341": "0feb684ca1519bc2f0fce5cc6b360eec50f84475", "1350": "e55aa2f3647b3717471ca9bd11b8e07cf3e2c5bc", "1351": "3521df049116a01d8c49cbeeb1dc80eca5e5040c", "1358": "97a70ddcc39d838e56d4fc3791df84bc65f52387", "1359": "48da60342ba7c40b8a7ffafdd67f1bf425aadb92", "1360": "7b4bfd1381182eff629eed528f242cdef9e1756f", "1361": "6c150ac0f33d8923261c49f5f42a31331ab2a2a4", "1362": "2b7d2527d5ec6d25ac527502947b425915c7d32c", "1363": "7f6f44c800b64fae03ecc28b80a6ab7fe7c05daa", "1364": "1c5ffa640a3e1a435af41860b2b8f8cb8b46039e", "1365": "2de4a362c0cc9b8199e4c84df0d1987944db178b", "1366": "f03d192f8523033d5e87ce716576a0d7e18fb5ab", "1367": "b3b0273556bef5b0cf8cc2cb1ac7e396f19373e7", "1368": "3e40f149ecb9db5cfa3e77ca21d5e838ac54a2df", "1369": "25b02da68ecc17fc7041baa139cc8183b2dcd60c", "1370": "bae4f506a561fe4cc24128fe1013b175581716d7", "1371": "78d8f530f8d8520f5cf2794e29422522981be2b5", "1373": "0e6227086bfd1aad3d5b957d6d3f04448383d170", "1374": "6b222463e23d77493909b173d1b39533b7212de1", "1375": "eafe3d929eee05fe0e2f882891cd4ff429703995", "1376": "97783313bff19689c4a74a0009c4127235165e91", "1377": "520919c89a7dcfb643c6b1a134c86b89e3da3c74", "1378": "15371ea4e6114e2608623466fac09abb9e614bf7", "1379": "24dfb5473caed808b641858f744a3ce73b181f05", "1380": "fe9de294580526e921102fbdedda736f20596fc7", "1382": "9e30f6218f824eb24a636f8f4294756b791c82c5", "1383": "66a4bdcdd64cfbc81216071787adf586dd2a17c5", "1384": "082867cb30edb514a726172f66e75d37f9ceedc5", "1385": "589152fba84ee7a3e834aabfe30be4f2abb5b2b8", "1386": "059a1a31f420f77fb47ae2c9a86fd0fbcef733b8", "1387": "182b6e15dcfcca5a4a1344ef4ab554b902b860d4", "1388": "0dc8101137423678758be072852d3e1555c3a23e", "1389": "4f3ce004cbed8613b7600b7ffecc5152b51c8388", "1390": "60d54597194097a9bcfae4ff95ad443be08779a7", "1391": "12fe44694f4081d79bce3cdac800d4bb17b1aa84", "1392": "0e7e2865bef00a7dba442f970e00050579073f86", "1393": "f951fe2686047121e57000324a611d7bf736bea8", "1395": "62dac1556fc12b4f564d9cf6da08f28eadd111af", "1396": "957668278e138971e1b3b7df2359ba42a511f77c", "1397": "c730b5b862b1f6bd1b8895f1ad0b6411056509bd", "1398": "00edde3de4f240fa962a69e1f6e83aaafbfad1f9", "1399": "329bd8c648297d21649c47c08c8d6ab4a10822b4", "1400": "79ef81dc4db58f950ac22b0463a543dbe8625a09", "1401": "7c45ec4ceecaebd1b1648029e9287f875dd4253c", "1402": "e353b0bfb244aabe915c0be1e079eeeb619263b5", "1403": "103ef9354fb4ce126fef3235eb4ca8d2a9693cd1", "1404": "64f5b11542f9dcd59894ed9dc6f85d66d11b0cc3", "1405": "e9bd56e8bee23ae5f743151336dcd573b90b4bc8", "1406": "cfdc5500ffe533788334d863fd6f58c51383672d", "1407": "9b3c24a1df9a8f6087b959049f6bbb3ff9028de5", "1408": "ee250526333ecb78b90267a8da28f9cdeaedd9e8", "1409": "01e41d376bb176783e43e0183a05dcff8355790e", "1412": "62c79e5829d89dbfd260d3f5eb9d17f880f01553", "1413": "a9d9182fd2bedcd8843e1be34e6a5a65adfde85a", "1414": "300e121e1be47ecfbabba78f077851a9c3b0772c", "1415": "7b37536b67b8410d2d890b8ee8bf38b05181aa61", "1417": "42091dbe1ea4b7b827cad4f53b2ef099eb43b4fb", "1418": "aa585fc0a5ae60f61f1752d78614e0951047d21e", "1419": "6b3c593924808d33a39e275f921f613f77139d06", "1420": "30f4c48db0d742898f629d129b5d1caa83091662", "1421": "b89fc53dbd7cd1eee783fed1fba7b401d69b6ffd", "1422": "396757028afbd460fd6325fd2403636ed8482d56", "1423": "61b00e99ce7861a0fd767572be0d58600c065b53", "1424": "41a2086963e018ffe0ac90e6fea72d3577d463f3", "1425": "550d3e7c29a5f9d67a0e993bbabc1cbf88295ef1", "1426": "a9d0e41e47c40a34e2e146a4297b5c638375fe85", "1427": "d4b40e287863960d608d52131471a88a435bf844", "1428": "9b93303adf8d5c436ae267be4d950fc5e55139de", "1429": "652d968561d473599e90508078005c4fd00a1ba4", "1431": "b808afac25bed84595b874f28b6b3ce2407fbd0c", "1435": "3c2fe145cad45797356a4ec5fbd42e4dac57681a", "1436": "2612fb45f15350dc483ddde1c8d43c2d6a02731b", "1437": "9e544da9d823c3a4e76b256b741925af52a20742", "1439": "bb65a85ab82dd7fec5297b258956545f8b4adea7", "1441": "67d2bbe530f96a0aa5412f479e123da480ba2d99", "1443": "a158b0c2e684c9389691744c001192ce94ddc79d", "1444": "424cd56e295afafd64444420b92fcf89a82dd1ea", "1447": "9d98a4436e376fa1623af92f23153cde5b68a68b", "1448": "3c059f4d4030dc73594f277d8754918c698a2969", "1449": "91a076cc267caf549ff38cb532dd76c5e4e168ba", "1450": "85d51e3b748982dc9121026a4dfcbd9f5a1dc2fb", "1451": "b8c8607285c089bcab3a642dcebf6a36a1131e5c", "1452": "64c6c7c79be08e0d80584d077e692ef78ec519d4", "1453": "e0f71a6a76409222d499fe5f18b1337793c1e239", "1455": "db2ff1dc45575f2c70dff7aa00310cd680862c78", "1459": "63ec283d2d43391b07aa2f1a461c7afd30b54014", "1460": "c199cd14509440e58b2159a9ce7ffe5922734a91", "1462": "70d99a0dbf2b4e1e04367e473129b4fc42cdc9e5", "1463": "7c326235643bbfccd227adf3ea114a5310812ff8", "1464": "26541d3858515374a9963680ed76eb26a6e29666", "1465": "d8e14b8efdb1c435b2e26737850c6657f7324c3c", "1466": "4c596f222ec4276c9ec2f8abbe3f7271af394ecc", "1467": "7e76422da9d21a398794504c8faa918096663ba3", "1468": "fc9514f25712d8e86fbdbd2f7e37e64f3a10df40", "1469": "6dd7f840b5f9f9fdb3a7b6f9b15313f45801e474", "1478": "8eaf5e7cdfd4b565f93fc7d78669374bb39c6b83", "1479": "88841eed69f26d4bb28ce0ba8d3295e65022a17f", "1481": "5fddf31ff6b4833ad699eb0456fb30e06754bccb", "1489": "fe78116f28aa60b7734dad2a32b3f11bcea50e74", "1490": "bc01cf06d36c177af84e14e4554b47595d09c6ea", "1491": "983bbaddbfcf5b87ef047ed854f6427718e2183f", "1492": "24bf7e6114f2b95a4746e10db95a3e5609586bba", "1493": "d468fec99c57b70fb1bb5b4d4c12527f05e0c150", "1495": "23e604441258b22f6868fd53c6c8264e9a945a17", "1497": "96102f1ad9aa1848390e49f9abca6cef9fc5b67b", "1498": "67a39ebe8d068fc91d06adc30d80aaed7943b9ac", "1500": "f3d82f06136351367d820c383f08233491de6cef", "1501": "7bae43ec4f3e9666cefe7cc21745dc9deef6a6bd", "1502": "ddef2481c0d206c52dabe73fe8a95ad977067f3a", "1503": "023f1a167c54e907f5cafdfce919d40423a6d115", "1504": "726a90a696b01497355373db7def03782b7a3667", "1505": "6c9f6b9660e5d97f63eb7a3b8c50e9865d2e8746", "1506": "c8749fc9ba9b4f88b3f1d5cafe721daf64e4f9cb", "1507": "9f7ae176c936af7ce90391641b6be116eb376c63", "1508": "5f6ecb1963a7a4e9ca98906873dd4a8debf98061", "1509": "dab82654bf23be9acb340812fb8db511c9aadbcf", "1510": "f73ddd1b286a3c9bf532c1ef95a5820ec3edb090", "1511": "102e5f051097e37ade34f9f04e6feb7e98f68fb0", "1513": "ae7e3caa313f1219c781da4d483a4fa6ed2cecbf", "1514": "3f9b1c7410f708cb58d8c2ee489be29b769e21e7", "1515": "c92b4484a312fd7e3e726047cf8cbe2b0a4400e9", "1516": "98e88b6b578d428f0d91eb3be66a02c913e06b48", "1518": "7a5f09b75adfd8ff4d34f8c6501db7d5bd25e35b", "1519": "2d65d634eaeaaf648e52c0ed0592563bae125a47", "1520": "5cb58364ff11163ba099074af43f785d14d4029e", "1521": "32661a821ae671cc56578f4c4a698c8c0bd8b1a3", "1523": "67a0f0281e1554f7f7db4ed918689cafb249bec6", "1524": "5ac230c30b07e326d50acc458f84460359b65c8d", "1527": "9e31113188e6451d1911c7fc3836232e5eb485bb", "1529": "f9ea93b00b5d81606dd0d763e302526a03f16004", "1530": "44ff99a8bc9dc75b189cc52d598239e43361b490", "1531": "4973d7f8f3f1b9f583ebdc7cf07dce05c75872bf", "1532": "e792b7b21e4b77f699fbd9929b8b6dbf082fd511", "1534": "5c926822abbfcc03fd5c93d4ad6a2519ee34a3e7", "1535": "d09082c78ee575257016eef41d50ab73a9f0b1f7", "1536": "caad58bb27ccabd8fcef3f8bf3b62c1d4320e8cf", "1538": "79c9a7605ee3643d292ece70b4d120660cdbe3b7", "1539": "c62d676017f4d405a58fd98db16987baa662f785", "1541": "0e9a3638143e46961c9f349433d41042c7c4bf8c", "1542": "70da00548756d362632b0924f2e0889c5aefd7a9", "1543": "7ff4b4a8c1e8b7d05201ef4da52d59c71fa6e63d", "1544": "91d1865166bd7e21558a1eafe4da3ce22ab42586", "1548": "db6c1186d3454974f6d9a488a5e0846a25cdead2", "1555": "37c3ee53ff81fedc97d7ce6a353e19e1ece14135", "1557": "a04885b4a401f7c8a3bccf4e5f029086d537f3cf", "1559": "159bc59f3d39c0b9f5fe8cdd780885d4462451b2", "1561": "38909e54a4332d15b69ab24dc313c9e039761930", "1563": "946d88ed302e0fe5a27a50871bff965014dcaf82", "1567": "e9d463114b63f1f6e2ec96575f179a99fd24f030", "1568": "e9b2136975a4cf0e88dd96028514879188de735c", "1569": "7138857887324bcf0599af8e09dd9b0074b168f5", "1570": "2947a13f832a0d06da9fdfa0cfea047dbedf7d41", "1571": "efc06bf14775b63bd1d8e2fbe4c5c534348f0352", "1575": "76a66fb0b28a64747eec8ab5d10ca5e349b494e4", "1576": "a7f92b8e46bcc54931896d681d06373a9706376f", "1578": "4ad6edf281fac10d0f02082b4b62c874eb079479", "1580": "91902de3b21c75638d0db8b45402226e5368fa84", "1581": "b23590a506758bc1531ec1d4b4ffcf77384d3373", "1582": "75f13762edab0660edf384031406c999070dd24c", "1583": "c870641c689829b2ba4d2f309ca0927d3a921efa", "1585": "37c91962e745d1db4a62296dac6e7a0cf71e0a3a", "1586": "3d2ec919e9c3dbb32c3b2ae38e9551fc52e937ff", "1587": "ca3254ff952feab91ce51cf8f94929b6bcf06c40", "1609": "e58d9c47ee6255470bb5b8f746ed684427d401b0", "1611": "a3c25646b2022b500aecb9140969547961f85d02", "1612": "dee245dcd4914833d8fd71e221907f6043b48bb8", "1613": "d73b393da5b317dbb4c0c41b5fb9c2a5054237c0", "1614": "be2b57c0aa4612fc378b06a19052674437bd382d", "1615": "a7c47e920c761ccd421ac50ac24bb89587ece12a", "1618": "d53b52a233fc8a849670262ed05596ba46124e4c", "1620": "a32ead86d6a92db7665f79318b0152c28e576a39", "1621": "2b3fd17d032afbc6d512309b7a52dc75d6257e31", "1625": "397ef0725d2bfda14f572658d377471d0f3c1cf4", "1628": "0a9cf6d8fa0ab11a9177e561a5be9a96aeadf758", "1634": "8849d7b19869de79ec875f48b0b3c310cea6dfbf", "1642": "1945a84a5fb962b4eb8e2963f14b814a8e344d7b", "1644": "bbbc61ad295c089802e2b12562ff1e959cb57c29", "1646": "d5a51d6ff2c601089fcb57f4764020782412968e", "1647": "883383666df5f17d90ef3681be5f79889b18be12", "1649": "f51a2fdcafc0d243b04ab559ca3eee78455afd6a", "1650": "08b1abe49af3491c652edf3df26af4ca121aa1d8"}, "revision_to_date": {"953": 1714565416000, "954": 1714495986000, "956": 1715273959000, "957": 1715370783000, "958": 1715672695000, "960": 1715777261000, "961": 1712846843000, "963": 1715950456000, "966": 1716292993000, "967": 1716453922000, "969": 1716997758000, "970": 1717087554000, "971": 1717170495000, "976": 1717414433000, "977": 1717505162000, "981": 1717592919000, "982": 1718036933000, "985": 1718120600000, "988": 1718196636000, "989": 1718291838000, "992": 1718729150000, "993": 1718792786000, "998": 1718980969000, "1003": 1719326066000, "1004": 1719403203000, "1005": 1719590331000, "1006": 1719857742000, "1007": 1719997980000, "1009": 1720163872000, "1010": 1720442209000, "1012": 1720715859000, "1013": 1720770981000, "1015": 1721060703000, "1017": 1721117007000, "1022": 1721321907000, "1026": 1721744859000, "1038": 1721812779000, "1040": 1722001245000, "1041": 1722271303000, "1045": 1722346568000, "1049": 1722432803000, "1052": 1722514049000, "1053": 1722602989000, "1057": 1722855272000, "1061": 1722440540000, "1062": 1723236605000, "1065": 1723308188000, "1068": 1723834000000, "1069": 1724088393000, "1071": 1724164405000, "1075": 1724252182000, "1079": 1719933161000, "1081": 1724417246000, "1085": 1724832448000, "1089": 1724840138000, "1092": 1725286457000, "1097": 1725284399000, "1100": 1725474667000, "1102": 1724658986000, "1103": 1725554020000, "1105": 1725887807000, "1108": 1725973036000, "1110": 1725979620000, "1111": 1726248025000, "1112": 1726508701000, "1113": 1727080005000, "1114": 1727161687000, "1115": 1727339498000, "1116": 1727437221000, "1117": 1727688292000, "1119": 1727781021000, "1122": 1727792229000, "1123": 1728043587000, "1124": 1728563742000, "1132": 1728660284000, "1136": 1728639898000, "1138": 1726151644000, "1142": 1729100594000, "1143": 1729173033000, "1144": 1726151644000, "1145": 1729669684000, "1149": 1729759206000, "1150": 1729854933000, "1151": 1729878319000, "1153": 1730119639000, "1161": 1730122298000, "1162": 1728899271000, "1163": 1730221470000, "1164": 1730281261000, "1165": 1730296936000, "1167": 1730377229000, "1168": 1730382436000, "1170": 1730799065000, "1173": 1730812670000, "1174": 1730477115000, "1176": 1730822184000, "1180": 1730819209000, "1182": 1730889307000, "1183": 1730972004000, "1184": 1730989718000, "1185": 1731075419000, "1186": 1731147566000, "1188": 1731675638000, "1189": 1731941827000, "1190": 1732023861000, "1191": 1732184398000, "1192": 1731499132000, "1194": 1733147084000, "1195": 1733167033000, "1197": 1733153149000, "1198": 1733234132000, "1204": 1733236736000, "1209": 1729765965000, "1210": 1733391146000, "1211": 1733403494000, "1212": 1733489089000, "1213": 1733511041000, "1214": 1733737041000, "1220": 1733740658000, "1221": 1733485963000, "1222": 1733753280000, "1228": 1733413595000, "1229": 1733811525000, "1230": 1733846901000, "1231": 1727439740000, "1235": 1733983014000, "1238": 1734009601000, "1239": 1734347040000, "1240": 1734360808000, "1241": 1734425916000, "1242": 1734432891000, "1243": 1734442108000, "1244": 1734514722000, "1245": 1734366030000, "1246": 1734618168000, "1247": 1734710845000, "1248": 1734729346000, "1251": 1735302468000, "1252": 1735808218000, "1253": 1735835555000, "1254": 1736178475000, "1257": 1736256881000, "1258": 1731584115000, "1259": 1736497390000, "1262": 1736869813000, "1264": 1736934927000, "1265": 1736960725000, "1268": 1737043225000, "1270": 1737111923000, "1271": 1734373221000, "1272": 1737467184000, "1273": 1737983940000, "1276": 1737565214000, "1278": 1734597155000, "1279": 1738015423000, "1280": 1737736098000, "1283": 1738076743000, "1284": 1738142611000, "1287": 1738172864000, "1288": 1738247248000, "1289": 1738315232000, "1291": 1738319705000, "1292": 1738566021000, "1293": 1738337516000, "1294": 1738599826000, "1295": 1738676519000, "1296": 1738760860000, "1300": 1738766871000, "1301": 1738584125000, "1308": 1738829031000, "1310": 1738860789000, "1311": 1738927717000, "1313": 1738942204000, "1314": 1738317412000, "1315": 1739478801000, "1316": 1739552725000, "1317": 1739785595000, "1318": 1739806727000, "1319": 1739814451000, "1320": 1739888579000, "1321": 1739957343000, "1322": 1739987173000, "1324": 1740046484000, "1334": 1740390135000, "1336": 1740475154000, "1337": 1740496276000, "1338": 1740558239000, "1339": 1740647925000, "1341": 1740750316000, "1350": 1741001991000, "1351": 1741191547000, "1358": 1741172665000, "1359": 1741357088000, "1360": 1741361198000, "1361": 1741339895000, "1362": 1741171230000, "1363": 1741641876000, "1364": 1741686107000, "1365": 1741689100000, "1366": 1741693500000, "1367": 1741697509000, "1368": 1741793323000, "1369": 1741844613000, "1370": 1741856758000, "1371": 1741863938000, "1373": 1741801382000, "1374": 1741879589000, "1375": 1741940081000, "1376": 1741957602000, "1377": 1741963436000, "1378": 1742202033000, "1379": 1742299666000, "1380": 1741877825000, "1382": 1742379446000, "1383": 1742456797000, "1384": 1741347887000, "1385": 1742573321000, "1386": 1742803048000, "1387": 1742814008000, "1388": 1742817001000, "1389": 1742912071000, "1390": 1743091673000, "1391": 1743095812000, "1392": 1743153261000, "1393": 1743175307000, "1395": 1743179441000, "1396": 1743490697000, "1397": 1743662600000, "1398": 1743669885000, "1399": 1743671775000, "1400": 1743751741000, "1401": 1744005300000, "1402": 1744092484000, "1403": 1744126269000, "1404": 1744194458000, "1405": 1744204792000, "1406": 1744238015000, "1407": 1744286626000, "1408": 1744293336000, "1409": 1744297055000, "1412": 1744360488000, "1413": 1744368417000, "1414": 1744369656000, "1415": 1744705503000, "1417": 1744730027000, "1418": 1744791011000, "1419": 1744803177000, "1420": 1744805310000, "1421": 1744806956000, "1422": 1744882795000, "1423": 1744895081000, "1424": 1745407886000, "1425": 1745505921000, "1426": 1745945048000, "1427": 1746180810000, "1428": 1746196158000, "1429": 1746234224000, "1431": 1746794777000, "1435": 1747040235000, "1436": 1747053560000, "1437": 1747133153000, "1439": 1747392078000, "1441": 1747405237000, "1443": 1747657731000, "1444": 1747728582000, "1447": 1747847008000, "1448": 1747903829000, "1449": 1748018807000, "1450": 1748339648000, "1451": 1748414056000, "1452": 1748430418000, "1453": 1748506374000, "1455": 1748583013000, "1459": 1748599094000, "1460": 1748602522000, "1462": 1748857286000, "1463": 1748966227000, "1464": 1749128783000, "1465": 1749192520000, "1466": 1749193648000, "1467": 1749213171000, "1468": 1749215985000, "1469": 1749482348000, "1478": 1750071612000, "1479": 1750087450000, "1481": 1750084411000, "1489": 1749812789000, "1490": 1750228519000, "1491": 1750239628000, "1492": 1750252294000, "1493": 1750318533000, "1495": 1750413753000, "1497": 1750666993000, "1498": 1750675720000, "1500": 1750684699000, "1501": 1750773951000, "1502": 1750924484000, "1503": 1750944576000, "1504": 1751269245000, "1505": 1751272216000, "1506": 1751286298000, "1507": 1751443437000, "1508": 1751519461000, "1509": 1751866644000, "1510": 1751882141000, "1511": 1751957453000, "1513": 1752056621000, "1514": 1752141097000, "1515": 1752147867000, "1516": 1752242863000, "1518": 1752247139000, "1519": 1752339415000, "1520": 1752501287000, "1521": 1752511574000, "1523": 1752590466000, "1524": 1752772016000, "1527": 1751974504000, "1529": 1752849558000, "1530": 1753084275000, "1531": 1753091752000, "1532": 1753100400000, "1534": 1753170633000, "1535": 1753186005000, "1536": 1753191655000, "1538": 1753197730000, "1539": 1753269354000, "1541": 1753190374000, "1542": 1753342065000, "1543": 1753346987000, "1544": 1753360267000, "1548": 1753280658000, "1555": 1753437216000, "1557": 1753446649000, "1559": 1753685816000, "1561": 1753695451000, "1563": 1753702168000, "1567": 1753707218000, "1568": 1753760558000, "1569": 1753767095000, "1570": 1753767905000, "1571": 1753768114000, "1575": 1753770663000, "1576": 1753770969000, "1578": 1753771726000, "1580": 1753776376000, "1581": 1753777568000, "1582": 1753780302000, "1583": 1753784053000, "1585": 1753789744000, "1586": 1753276706000, "1587": 1753797899000, "1609": 1753966382000, "1611": 1754036682000, "1612": 1754040139000, "1613": 1754058617000, "1614": 1754300123000, "1615": 1754305882000, "1618": 1754321975000, "1620": 1754387002000, "1621": 1754390870000, "1625": 1754468821000, "1628": 1754470318000, "1634": 1754487457000, "1642": 1754582246000, "1644": 1754656037000, "1646": 1754675740000, "1647": 1749827940000, "1649": 1754894826000, "1650": 1754900234000}, "params": {"machine": ["ArcticDB-Medium-Runner"], "python": ["3.11", "3.6"], "version": [1, null], "branch": ["master"]}, "graph_param_list": [{"machine": "ArcticDB-Medium-Runner", "python": "3.6", "branch": "master", "version": null}, {"machine": "ArcticDB-Medium-Runner", "python": "3.11", "branch": "master", "version": null}], "benchmarks": {"basic_functions.BasicFunctions.peakmem_read": {"code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "5350d84365ea3a972c4aec4c3b1a7eb48520c116eba1aca60f68671c7ceb7db8"}, "basic_functions.BasicFunctions.peakmem_read_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "050f863cc095eb3c53b80d320695ae92e69050877742108775b5fad2709eee10"}, "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "c7b3ab2fb3717f309d2627beb6032db5459cbf5648aa1a0e091290c88af77250"}, "basic_functions.BasicFunctions.peakmem_read_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_columns", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "047f4275a35a623a40ef0a98f437cfdabdf9f7ff5f6c29e6f4246e8bcc11a36a"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "eb643d25455a8fe0f500a3e587b0ce0002907f43ae6e39d7775aaac1e7a2fe93"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0ae28943a0c68ba7c67616ce2199fe35bf51d2cdbe2d2b2c962c96664bcefb19"}, "basic_functions.BasicFunctions.peakmem_write": {"code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "07a76278d58a355f560d7d5bfd1ca0457cbde13ae5327909587acb001b06c45f"}, "basic_functions.BasicFunctions.peakmem_write_short_wide": {"code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "6173c4098439ead864ff0ff9389f68ca2fce88c1454e7d6a162185f1aa6f8aab"}, "basic_functions.BasicFunctions.peakmem_write_staged": {"code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_staged", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "bea653ed2c0175761c392b80884986c1fbe5ab70bdde3d2114e5f92a84f7da47"}, "basic_functions.BasicFunctions.time_read": {"code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a70c622c0ad6fd9d008c2b36342666f99b8c14f75d14c859d619cf2cc37e34ad", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_short_wide": {"code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8527c0bb585ee2f0d4d115dd110b8911b243e6ddd28139771dcdd158fc4ad124", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_ultra_short_wide": {"code": "class BasicFunctions:\n    def time_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_ultra_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "591d23399ad85009e024d4ee18eb2b5a4a844d6ea0b937c49adee40d8f24bc8e", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_columns": {"code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_columns", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "19bbbb9d179141472b122afd3ad90408e2a3546eaeb640514c2c6b1fca9fa686", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f59257c55a9106758070701cec081f4bf021bf1cbcb6d698dcf694419cd9bddc", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2c5d17c2d1bd6212c16a5127aea1ca379224ef610b06111a93c5b80e04922f8d", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write": {"code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "54f11aa38218f73c7a34bec5c71fc51af8b57ff2ac64f2cf3db688dc462efcdf", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write_short_wide": {"code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9c7467ee5cd1c182f05a78de9dff71956edf647d369b0cdbcc0cc431f96e2c24", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write_staged": {"code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_staged", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:44", "timeout": 6000, "type": "time", "unit": "seconds", "version": "71e4fffc77870273e8186b7b0a8a275fd2062d2eec72a11834004b88c2bca07c", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.peakmem_read_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ad4186536870277e6d6b172d16c78e25ca0a24077b06d47c0ec85f2029105468"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "c3bfc7e448a19b3714478a4bf68abc2727b2377cb6e4ea40ef477da286053af0"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "5c6e176c14f36368bc30cfba9d19586bf9d32ebffa8f5e4d1f0644afe8879c4d"}, "basic_functions.BatchBasicFunctions.peakmem_update_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_update_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "35b7cab5345fc9d919975af82e8566fe696e0e5ebb823621d977ebfa37814db2"}, "basic_functions.BatchBasicFunctions.peakmem_write_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "20c4987d92ab25712c9af326e22a5253ec27eec3b8a162a4e56830b8e2889fd4"}, "basic_functions.BatchBasicFunctions.time_read_batch": {"code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "91fcac5344d14a496a34cbd4cafc3b187e9579d1912b6156ecae8087d80df7b2", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_pure": {"code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "bd42aec09d03cbf1c6f55d86f114ea0f5627950c8a255ce64ccdcc8cedaa029a", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e11e8495b1ac21d015735c6d9bd38a219ad0c386063557efa4673c881155d331", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b90235fb10cb19836b85cf7de7316d780ebd9b4d015e906fbbb3840a556f55b9", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_update_batch": {"code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_update_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "15c4acc46acc92479e795e0cbf8664b0a75afd63f626f799457d10fde36d928b", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_write_batch": {"code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_write_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:167", "timeout": 6000, "type": "time", "unit": "seconds", "version": "71706b100e9d76919fe5a09b6dd6d60999519ea4acd92879edbbf7a95ee919a0", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_large": {"code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_large", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e0f0f9a0466ec28333d3f62c9573349dad1a2196b0a689aa36dc84a278dd85f9", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_short_wide": {"code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "24aaf8279ce51c426c441a72627f7e92355854d79b2696ce28f11bdded0bb8ff", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_single": {"code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "64309481ad0e09f83ffdf23907dfefef6ae5693a19632f0e65d559d06fc67c7a", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete": {"code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "83a387a99e2752c62d1bfcaee3bc60d519a3476cb6eb84b550622bc4aed573b2", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_multiple_versions": {"code": "class ModificationFunctions:\n    def time_delete_multiple_versions(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym_delete_multiple\", list(range(99)))\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_multiple_versions", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e83fb6e5a7df255f8958f7a788591c2ce139b27f02f1b7452ef480b61d656a0f", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_over_time": {"code": "class ModificationFunctions:\n    def time_delete_over_time(self, lad: LargeAppendDataModify, rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_over_time", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "0b6c95b8c3be0ec971f33cb33adff68cdd21a6a45d05eddfacf93070e4466c34", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_short_wide": {"code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4ab9a6b853b21803a718b9482c8da6bb608c9ed0f17397344dc916f194c9ce39", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_half": {"code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_half", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5e6dc60fc2e1e05d60ee28e519c20a4287636ebcd42521b96b555a50b4b5e4a6", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_short_wide": {"code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "0d7782f15f49e0949eff17e5b713b5bf944160c3a58972f65e72c981e5647a12", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_single": {"code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "623f9f87c5901e77a9bd521ed89d304ede6789595fbc3211cfe06706056c3241", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_upsert": {"code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_upsert", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:316", "timeout": 6000, "type": "time", "unit": "seconds", "version": "22856662392bae1926e4c8c152895fb9fe9ff465ad9ee0a9c3948ad02a92f4e7", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "4aaa19de6a81f69d2a57d3d00f2d06c2eaf3a68c4180c2e6bac0c01650a48645"}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "fb9dfb58c26e19e743c423f9a049ef0680bf6b7772019bc36e2b8f91b50ba77f"}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1c03264219ad059962b4deaa334a0c8574833075c87dd548898ce4172cf7f5cf"}, "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {"code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "d55ec77300186fc84bbc3d9402694f991fdaf329eda0750551d57425e41e5087"}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f3e937a52faf63c837949a6eb48debf02755ddd4b699c5c3800831a5cda9ad67", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5dc7443c6e30c3fe69bfc4f04cb28fb5aa503162cab5e204606a9f6e2233e581", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "time", "unit": "seconds", "version": "46c41f587be844def6cad1cefd995ea32c9719285172e8a0ecc3ad193b991247", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_readall": {"code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_readall", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:73", "timeout": 6000, "type": "time", "unit": "seconds", "version": "16923258b23421434453a4143833cd446747e065f3e982bfd61ed19a3ee0130f", "warmup_time": 0}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe": {"code": "class ComparisonBenchmarks:\n    def peakmem_create_dataframe(self, tpl):\n        df, dict = tpl\n        df = pd.DataFrame(dict)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:38", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "3fba994ac7997b310a49e3a6eaff3aac7aaea9072e7dce6025959b53c8e891c9"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic": {"code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_arctic(self, tpl):\n        self.lib.read(ComparisonBenchmarks.SYMBOL)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:38", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "3f0b5ba30cf1ba9ff3ac3a9e89aeea8f4b68a1f94cef660ead8f66845d61978b"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet": {"code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_parquet(self, tpl):\n        pd.read_parquet(self.path_to_read)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:38", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "ac09370986d4696bde103c09c29fca234c5f6de88f367e1f9d2cab6c939740b3"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic": {"code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_arctic(self, tpl):\n        df, dict = tpl\n        self.lib.write(\"symbol\", df)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:38", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "8f859b71fa3b8c3c4acefdc333d07f57aff7d9a897e0bd93096dbdc44f99a2e4"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet": {"code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_parquet(self, tpl):\n        df, dict = tpl\n        df.to_parquet(self.path, index=True)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:38", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "219e33422ea9ced5dc92ced39e0eea27dee1870cf991a82db139498f17446fe0"}, "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {"code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF", "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data", "param_names": ["param1"], "params": [["1000", "2000"]], "setup_cache_key": "finalize_staged_data:41", "timeout": 600, "type": "peakmemory", "unit": "bytes", "version": "9dcfdaf896125a0fe0d16b5538b5a8b556997064e107c8b58b93dc6e6f32d8b1"}, "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {"code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF", "min_run_count": 1, "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data", "number": 1, "param_names": ["param1"], "params": [["1000", "2000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "finalize_staged_data:41", "timeout": 600, "type": "time", "unit": "seconds", "version": "c3c02d1e2369dd420b2e241fc69c4c8872d31da89d0c19c1111d503a84fb9521", "warmup_time": 0}, "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {"code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF", "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data", "param_names": ["param1"], "params": [["1000", "2000"]], "setup_cache_key": "finalize_staged_data:92", "timeout": 600, "type": "peakmemory", "unit": "bytes", "version": "90cde854b0e3346d50d63ab29182811b92cd7fae6c4ce0be4011a62c534e5e0f"}, "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {"code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF", "min_run_count": 1, "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data", "number": 1, "param_names": ["param1"], "params": [["1000", "2000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "finalize_staged_data:92", "timeout": 600, "type": "time", "unit": "seconds", "version": "a7673a8f559a07772f7a7a8e105774090534c7eb1b644b2d6247e7b792645809", "warmup_time": 0}, "list_functions.ListFunctions.peakmem_list_symbols": {"code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_symbols", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:24", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "efa8557e59868203fde3f8d2921698b505ae7a1ce7ff442b3e4c9bebc9ce2771"}, "list_functions.ListFunctions.peakmem_list_versions": {"code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_versions", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:24", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "30457537b6ea77365ec0021b6f78a618dd0e990631d64cf0ae6b85baddca7081"}, "list_functions.ListFunctions.time_has_symbol": {"code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_has_symbol", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:24", "timeout": 6000, "type": "time", "unit": "seconds", "version": "00a6aba7cd18f9fbbfa18c85961d58a03a291bfe32bf033e8d7b88c7b960da90", "warmup_time": 0}, "list_functions.ListFunctions.time_list_symbols": {"code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_symbols", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:24", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7457ceb57b7adfda687387a4599ff60b20ecb6ef556b80329ad2e8ec433fbb17", "warmup_time": 0}, "list_functions.ListFunctions.time_list_versions": {"code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_versions", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:24", "timeout": 6000, "type": "time", "unit": "seconds", "version": "cc2c68ce66d0087882fffcb8be554f525c3f314c8693a37897d37cc18373f1ff", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {"code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")", "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list", "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "setup_cache_key": "list_snapshots:41", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "55c5726e8be0eafb5adfab9f83954dc1fd727249fc74a1df3d27bed647ad1040"}, "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {"code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")", "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta", "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "setup_cache_key": "list_snapshots:41", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "b7f4df120eb3ef915bcfd99cc75efe38896ffeafd5b6a8c3e5f3a8db67ccaad3"}, "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {"code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_snapshots:41", "timeout": 6000, "type": "time", "unit": "seconds", "version": "706e811eef8155e3494d1a30ae8920e15ae32e0b69826eaa92388d620a5bb4ff", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {"code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_snapshots:41", "timeout": 6000, "type": "time", "unit": "seconds", "version": "d4090a8af79a8cd45f87d87a807c4f344d86fa4e51e528809922b6a602ce06f9", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {"code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_snapshots:41", "timeout": 6000, "type": "time", "unit": "seconds", "version": "71446e73206e086160b0733fdec4689d0c813c036e061f99d6f4ef372dd49b3b", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1f8093c32e1c5195eb0efb1004c228524cb54aa35d8c79359b17fc91597391a6"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "fa0a87f2f2956453b825adcdb9e95be6a7e8887b2a66839923aa8a433e296e4e"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ce91e45ba6ec5f5dcd9499b423014b431774a7d81f07daa90d6c29cb8bc84d02"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "4cad6e9389f20fc4a168893003dff16e0577770525b847e71e3b97f0f9f5ecdd"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "91dfe91e2fd6e9d562d89d8aee902dbb5c2380f3cd0a11eb85229cb375a7ea0b"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "94edfd985cb9746d21b85be1c91e97423797af2faa7a3343ad1c3aa7f9fa4536"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e047abda334f31dda20959739f2a3816f4dc96c130db00ebb75f5adcb9c14999"}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "1fd26d5df8e3bd47278b0f1acca9528cc0dadba82788af6e3cfd1812058abef9", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a0f79b58b7744e63b2b7df3562f57094fa4ff3a111c172fbe0b03aec197afec8", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5206453b05dddbb325596b08eae2200fd2193c2e2382ae66982838b1cc97859b", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_projection": {"code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c7f842a915ebd3e278a9a5cea838835a804b463451ebec69829afe871adccfcc", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8972136efca70caee7530d031766c4653737a79d09b7c7badaaee274c1caa7da", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "17ef74af58c623de0ce47d10ad9d52ffc8a1b3c3bb2f57d1391dde34f4af4f29", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "509ffd471564124f5ea73eab19903e52e70eba728ea59b97ad6bd5b8544c2e60", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:26", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9a923014466d420b857d297f2a8a41983d03d0c3242559a8488a2a9a642440e1", "warmup_time": 0}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:63", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9d97dcd98574b9edb2038a9d43166c03fb90874813e5fac9c3a44b51194f3dd9", "warmup_time": 0}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:63", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b1364bf72e616201e384c0b7a9f18b03b078e22452929466a06b35fc64a91bd6", "warmup_time": 0}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:63", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8f27fb785c7b8b40220191dae6dbb120a49f55e011ae0f7cea6516a47e38c18a", "warmup_time": 0}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:63", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ed1d1ccb6458095a627788bfa2b53afa310ca8c8118a6405c91204724c865d6c", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "5f940b32e17b1e08e0e79df3ddb81fd60d6217ec6274ee308a3cccf5a90cc72f"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "e5f752bdf60df192471e9f0a0bb7ee74f3582679fd461247cda321614ecfc952"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "0bc60840654ec805851574b1f8ef76987cbd0ac99806d08abf47e7a5c415fd4c"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "e3de99f1307e75a7b5fddd8d7f3e4fba1975fda0f0f196603caa638b4cb3569f"}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "time", "unit": "seconds", "version": "919b109aa3f63f22826be5a5f1255dcd06e284fac7035d1d2b8446ef182d4f3f", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "time", "unit": "seconds", "version": "c4f2b10ea3bae71c069942dc0b9ed61b161076d0c9ed9e9a3eabdd56aa720675", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "time", "unit": "seconds", "version": "21629b37558919e369c6b23aab6179b57f09fc95247b5819b07cac4a46ce608c", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {"code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:53", "timeout": 1200, "type": "time", "unit": "seconds", "version": "7dc9bf11079cd6affdcf109a9d3f2ea057c2f52f63593fd9260004930ac7a6e6", "warmup_time": 0}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "6d29e295671e2ea22e0d30278ef69d9d7740a8e9c7dd806efa5d6144cc3647da"}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # measures base memory which need to be deducted from\n            # any measurements with actual operations\n            # see discussion above\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read )\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_read.read(self.s3_symbol)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "659f77bdc21378d3f9210e0f791c2cea56ca30f7a06644ec82d3d8cad9964efe"}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_write.write(self.s3_symbol, df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "6e7f2fc17ff2e3550a927ac3659eae946384ea1f88bfb66049eae9f7969b6f02"}, "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def time_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "min_run_count": 1, "name": "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe", "number": 2, "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "time", "unit": "seconds", "version": "f775e01172180c26b5d2a21e04b658cb3389adb8256920c58a20a2798f39131c", "warmup_time": 0}, "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data": {"code": "class AWSFinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache", "name": "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data", "param_names": ["num_chunks"], "params": [["500", "1000"]], "setup_cache_key": "real_finalize_staged_data:43", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "807052d5c2c0c054cc0b91e1255cb54af1e074c1e216974289db9fa905eff800"}, "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data": {"code": "class AWSFinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache", "min_run_count": 1, "name": "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data", "number": 1, "param_names": ["num_chunks"], "params": [["500", "1000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_finalize_staged_data:43", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e637474610e2fa47bd54a3077f46e8b35f94f808c4aadccbd01e8ff63d51dc06", "warmup_time": 0}, "real_list_operations.AWSListSymbols.peakmem_list_symbols": {"code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs", "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols", "param_names": ["num_syms"], "params": [["500", "1000"]], "setup_cache_key": "real_list_operations:51", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "fd9a84c3b5ca3f9971e5460376018e832367ebcb746eb52aff37606f622eff17"}, "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {"code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.has_symbol(\"250_sym\")\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting", "number": 1, "param_names": ["num_syms"], "params": [["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:51", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e61b7963339a5c66c99a44b8a5ed4246ce52e18c56de05ad969ccfe36e1007df", "warmup_time": 0}, "real_list_operations.AWSListSymbols.time_list_symbols": {"code": "class AWSListSymbols:\n    def time_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs", "min_run_count": 1, "name": "real_list_operations.AWSListSymbols.time_list_symbols", "number": 1, "param_names": ["num_syms"], "params": [["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:51", "timeout": 1200, "type": "time", "unit": "seconds", "version": "740a4dee3a32d8cb9246934e1860041a0a4d07cf870f17a851c804c95797d293", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots": {"code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "fbbf510c321854b58ec444401f123c69f5740b256525334c302218a2ae6d0066"}, "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata": {"code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "34c30a2205a1449369b92853def2c677da806e1fd0c092f8ff90a06fec22e7eb"}, "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {"code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "68626a85492c785d254c3253ac781ae868deafb7d32980f62984e011ac0c5f07"}, "real_list_operations.AWSVersionSymbols.time_list_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "0a97c0c02ba988159c2ba09b8e7f02ca34474bf249980aaea55c37d440d7721d", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata": {"code": "class AWSVersionSymbols:\n    def time_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "6221253e0b62c144c7223b4969b58a430a1717462bd939900b602a5481d24bc2", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions": {"code": "class AWSVersionSymbols:\n    def time_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "2f3663667131b7a0be1abbf0400dafceb3af124a460735034097a907a3f9bcb9", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {"code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "193b11a4617959d826ba0c91617e552bdffd44feb3b78ced90c5300c0c0450ec", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only_and_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True, skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d16cac6fad1d99375dbec55fcb0890d7444f076796e6952dab891f2d79b4b70c", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "3493206fe0748c6626346d9cb1573b137114e32a4a30dc7aa082ba90ccb3c57a", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {"code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(snapshot=last_snapshot_names_dict[num_syms])\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:129", "timeout": 1200, "type": "time", "unit": "seconds", "version": "2aff065897965f79c4a26f9107a7acda0fa16d88dbfb3ffc50cf98ca90570155", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {"code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {"code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {"code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {"code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {"code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {"code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07", "warmup_time": 0}, "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {"code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache", "min_run_count": 1, "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete", "number": 1, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:261", "timeout": 1200, "type": "time", "unit": "seconds", "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36", "warmup_time": 0}, "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {"code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache", "min_run_count": 1, "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time", "number": 1, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:261", "timeout": 1200, "type": "time", "unit": "seconds", "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_append_large": {"code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_append_large", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_append_single": {"code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_append_single", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_full": {"code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_full", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_half": {"code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_half", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_single": {"code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_single", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {"code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "9e0bfdbf626113f82fdfd1ffcd6e02ac70422c95277dc9b2b71b9eff44dd5844"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "ded8cd45317d8b9ef7d267bbe014125a0cb892b87d4f249a7b113c7fa8d09df0"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "1e538c9cadf80b7110caa89fd8da6c930281e5f0d8f0f77b874ff590cd359fcb"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "4a5c834b77a9e0290a0acc55ea4aad637b747af99e048bd0f7a953f8b5fc763f"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "9fcfddabe4a1ac18529dc03b6cc3aaad336249b652774a16aaba62e2132702b5"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "6f7f2edde2507709dc18b41fa471455f6689bc62c76f009646bcfa9337ca8485"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "56097ba0e08cd9b953d9731b60fc7c04ecba690f615098d26be69fc8cf6f105f"}, "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {"code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "5051b70ae6f5cbc9493af6ec64ea7f644db64a60d0a58f5de1930870d44c02a4", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {"code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ef199c9d78fc43c44984acaf08e9867a6efa89ab5b0012e51ecd4e5fd1fedce4", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_projection": {"code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "df07f61fb088929ad901e2fea6f172430f060daaadd6457d9eea51bad1129a8d", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "9720f704e40e9de6ae39cdab8db71d34c19b46fc05f4151e49ecebd50f01fd0d", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "b0ebeed0c3c098fafd4f9f1f1b4031dcddfeec4eb44ec0789a60f9b9ff3df04b", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "da26f9fa31e449bf2696324ee811ee3a949836935c265a28d8728bce3ca03579", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:66", "timeout": 1200, "type": "time", "unit": "seconds", "version": "7542c6a85b7d4791481f3106886009ee800f6fdfb132008b9f6fa60c6ed93848", "warmup_time": 0}, "real_read_write.AWSReadWrite.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "0689b459c6c7bb49e10edc6df8a37976cdefa6017472521a35896aeddf267ae0"}, "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "61215cb103adee49fa6f3f56e1f611dcd697f9b44e8f2be23d35b3c6b997883b"}, "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "285c5f2f9ba3da391ed62f3dfcf5e754258744e6ee152ccf26bf8a2ca571c972"}, "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "67423f3bf528cd9f5242633d93b484b558436e5c843c939d5071217f435ef906"}, "real_read_write.AWSReadWrite.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_write", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "0625aa02db843284e2bd88936c72d6ed89e4f8399179c6230fffb4e15096e141"}, "real_read_write.AWSReadWrite.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_write_staged", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "7ba6fea1dcf0c0e751683e095293ce820525ee930e234bbd516af15b7636d80f"}, "real_read_write.AWSReadWrite.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "612120fc41c174f1a09dadd9b53367c87432990752569e4fbfccc84a8d0094b6", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_column_float", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "f2264b620c766af4a90a0eb7e2e06b5fbb47f300449e8ed6a8e3a265e695b9ff", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "8c7fb09e61e2d7bd0f229b2ca10e61cde1f54eadb187d6f9f8136eddf32d7b1b", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "84f2f904f9ca0aa633bec423ac9385f780529edd8cf618e13690b07781a90b23", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_write", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "2e85d7bf669c8aab495b34003fc9d5f536e37ee77b0154c98debb39612a1898e", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_write_staged", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:79", "timeout": 1200, "type": "time", "unit": "seconds", "version": "aa2c3915c2cc65992b8d64669e2bc74ccfbd4d83469b0eb253739b11211b46be", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "bcdfc7aa551bc9ae43c6bf8dbb5ea2b9b92e03e7e6da8487ac110b302443beb4"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "d8263d005135750e39a3e13bd7de6702642ec7991a7dfde787b4bdba7a47125c"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "febc2581d6ee0495f21fe39fd5b9e139ee3784074ebef2305ea52a167d60a449"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "ac2ade0b551d6e6eca41a7b098855fc013f95c1ad95a97d15fcb716a99784bca"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "ebfae974146eb06e3015c78f663185f041fecba4f2d21265a95a0f8e0b3ed114"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "8e0b84e936c8b20c9ab1100d19b36f06abe0690a3d42f1a121d487556d98fa66"}, "real_read_write.AWSReadWriteWithQueryStats.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "02de728e3f80213fda1fc979b4aaf61786cd350dc31f266000478ce15d5c04e0", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e730587e51a99d028e7438d4874d1e0b1f098b49ec2773dbabf9b06d7c562315", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "69b8202ed5e2e62aefd161b99db3b55667643789c66fc99599abf2649c144ab7", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "35c60f81beba373aaf062739dc364a9835f7f7bcccd9ac4ae7187f2f97a6a0c5", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_write", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "dc443b21285eae3a4009ce860f6d4be6cb0164245da60aad1b63ec6fecd5d4f8", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:228", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ed3fedef28f86763035af2a64963966bf6724343de519001c9ac1a4a72d84928", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "6b035607aea2f64b0266d23fd98a15543efda0abc94c68902099d2525db7050d"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"}, "real_read_write.AWSWideDataFrameTests.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_write", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"}, "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"}, "real_read_write.AWSWideDataFrameTests.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "7f4881ccccbe07a5dc1f77fcf1e75aaa331f0f76a590eda75c876742eb30c613", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_write", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_write_staged", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:200", "timeout": 1200, "type": "time", "unit": "seconds", "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978", "warmup_time": 0}, "resample.Resample.peakmem_resample": {"code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "name": "resample.Resample.peakmem_resample", "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "setup_cache_key": "resample:38", "type": "peakmemory", "unit": "bytes", "version": "760c9d62e17a5467f1e93abb258d89057e8fdf9ee67d98ceb376e731157a4d2e"}, "resample.Resample.time_resample": {"code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "min_run_count": 2, "name": "resample.Resample.time_resample", "number": 5, "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "resample:38", "type": "time", "unit": "seconds", "version": "1381d2db90e66cb5cd04febf62398827a3ac9928795eaced908daec35d5c0c31", "warmup_time": 0}, "resample.ResampleWide.peakmem_resample_wide": {"code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)", "name": "resample.ResampleWide.peakmem_resample_wide", "param_names": [], "params": [], "setup_cache_key": "resample:106", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"}, "resample.ResampleWide.time_resample_wide": {"code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)", "min_run_count": 2, "name": "resample.ResampleWide.time_resample_wide", "number": 5, "param_names": [], "params": [], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "resample:106", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2", "warmup_time": 0}, "version_chain.IterateVersionChain.time_list_undeleted_versions": {"code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_list_undeleted_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:37", "timeout": 6000, "type": "time", "unit": "seconds", "version": "6bdd43d7f191d2bbbd30ef740909969e25cbe1cec77f1755c5c3ba58a77f2b88", "warmup_time": 0}, "version_chain.IterateVersionChain.time_load_all_versions": {"code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_load_all_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:37", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c40fe3123db9e5d6fdf5f35caecaf42d266328deb78c237e293096ae3a4bcf98", "warmup_time": 0}, "version_chain.IterateVersionChain.time_read_alternating": {"code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_alternating", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:37", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ec1a61c37c4cc7317cfafe554f3eeb7fe2a426068ec412c1d7c6b78f510f6c45", "warmup_time": 0}, "version_chain.IterateVersionChain.time_read_from_epoch": {"code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_from_epoch", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:37", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5c6aace0b39c7a75f064a61c182cbbb42a35f0e0ee46546579bc641e68dc954a", "warmup_time": 0}, "version_chain.IterateVersionChain.time_read_v0": {"code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_v0", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:37", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4bf693e490128c1cff7500c93799432e7bf150925d3714757219604aa7fa5e9c", "warmup_time": 0}}, "machines": {"ArcticDB-Medium-Runner": {"machine": "ArcticDB-Medium-Runner", "python": "3.11", "version": 1}}, "tags": {}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}